# IMporting SQL libraries
from pyspark.sql.types import *
# from pyspark.sql.functions import * 
from pyspark.sql.functions import sum as _sum
csv = spark.sql('select * from azureData4')
csv.createTempView('mytable3')
csv.columns
spark.sql('select count(*) from mytable3').show()
# IMporting libraries
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer
from pyspark.ml.classification import LogisticRegression

# regular expression tokenizer
regexTokenizer = RegexTokenizer(inputCol="processed_text", outputCol="words", pattern="\\s+")

# stop words
add_stopwords = ["http","https","amp","rt","t","c","can", # standard stop words
     "#keithlamontscott","#charlotteprotest","#charlotteriots","#keithscott"] # keywords used to pull data)
stopwordsRemover = StopWordsRemover(inputCol="words", outputCol="filtered").setStopWords(add_stopwords)

# bag of words count
# Here we created bag of words as 10000 beecause more than that will take hours to compute and sometimes fails to.
countVectors = CountVectorizer(inputCol="filtered", outputCol="features", vocabSize=10000, minDF=5)
# Creating pipeline of basic data cleaning and creating dataset with all features.
from pyspark.ml import Pipeline

pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])

# Fit the pipeline to training documents.
pipelineFit = pipeline.fit(csv)
# Transform dataset with new pipelined features.
dataset = pipelineFit.transform(csv)
dataset.dtypes

dataset.show()

d = dataset.select('features')

d.show()
d.createOrReplaceTempView('mt')
a = spark.sql('select * from mt limit 1000')
# Splitting data for K-Means
(trainingData, testData) = a.randomSplit([0.75, 0.25], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

from pyspark.ml.clustering import KMeans

# Trains a k-means model.
kmeans = KMeans().setK(2).setSeed(1)
modelKM = kmeans.fit(trainingData)

# Make predictions
predictionsKM = modelKM.transform(testData)

# For Pyspark CLI
wssse = modelKM.computeCost(predictionsKM)
print("Within Set Sum of Squared Errors = " + str(wssse))

# Evaluator for K-Means
from pyspark.ml.evaluation import ClusteringEvaluator
# Evaluate clustering by computing Silhouette score
evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictionsKM)
print("Silhouette with squared euclidean distance = " + str(silhouette))



# Print cluster centers
# Shows the result.
centers = modelKM.clusterCenters()
print("Cluster Centers: ")
for center in centers:
    print(center)

predictionsKM.show()


# Splitting data for K-Means
(trainingData, testData) = a.randomSplit([0.75, 0.25], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

from pyspark.ml.clustering import BisectingKMeans

# Trains a bisecting k-means model.
bkm = BisectingKMeans().setK(2).setSeed(1)
modelBKM = bkm.fit(trainingData)

# Make predictions
predictions = modelBKM.transform(testData)

predictions.show()

# g by computing Silhouette score
evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))

dataset = dataset.select('processed_text', 'words', 'filtered', 'features')

dataset.createOrReplaceTempView('my')

dataset = spark.sql('select * from my limit 1000')

# Trains a bisecting k-means model.
bkm = BisectingKMeans().setK(2).setSeed(1)
modelBKM = bkm.fit(dataset)
# Make predictions
predictions = modelBKM.transform(dataset)

dataset = predictions


(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

trainingData.dtypes

from pyspark.sql.functions import *
trainingData = trainingData.select(col('prediction').alias('y'), col('features'))

from pyspark.ml.classification import LogisticRegression
# Build the model
lr = LogisticRegression(labelCol='y', maxIter=20, regParam=0.3, elasticNetParam=0, family = "binomial")

# Train model with Training Data
lrModel = lr.fit(trainingData)

testData = testData.select(col('prediction').alias('y'), col('features'))

trainingSummary = lrModel.summary

import matplotlib.pyplot as plt
objectiveHistory = trainingSummary.objectiveHistory
plt.plot(objectiveHistory)
plt.ylabel('Objective Function')
plt.xlabel('Iteration')
plt.show()

print("areaUnderROC: " + str(trainingSummary.areaUnderROC))

import matplotlib.pyplot as plt
import numpy as np

beta = np.sort(lrModel.coefficients)

plt.plot(beta)
plt.ylabel('Beta Coefficients')
plt.show()

predictions = lrModel.transform(testData)

from pyspark.sql.functions import col
see = predictions.select('y', col('y').alias('label'), 'rawPrediction')

from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
print("Test: Area Under ROC: " + str(evaluator.evaluate(see, {evaluator.metricName: "areaUnderROC"})))

(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

trainingData = trainingData.select(col('prediction').alias('y'), col('features'))

from pyspark.ml.classification import DecisionTreeClassifier

# Create initial Decision Tree Model
dt = DecisionTreeClassifier(labelCol="y", featuresCol="features", maxDepth=3)
# Only for cli
# dt = DecisionTreeClassifier(labelCol="y", featuresCol="features")
# Train model with Training Data
dtModel = dt.fit(trainingData)

testData = testData.select(col('prediction').alias('y'), col('features'))

# Evaluate model
from pyspark.ml.evaluation import BinaryClassificationEvaluator
predictions = dtModel.transform(testData)

predictions.show()

from pyspark.sql.functions import col
see = predictions.select('y', col('y').alias('label'), 'rawPrediction')

see.show()

evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
print("Test: Area Under ROC: " + str(evaluator.evaluate(see, {evaluator.metricName: "areaUnderROC"})))

print ("numNodes = ", dtModel.numNodes)
print ("depth = ", dtModel.depth)

(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

trainingData = trainingData.select(col('prediction').alias('y'), col('features'))

from pyspark.ml.classification import RandomForestClassifier

# Create an initial RandomForest model.
rf = RandomForestClassifier(labelCol="y", \
                            featuresCol="features")

# Train model with Training Data
rfModel = rf.fit(trainingData)

testData = testData.select(col('prediction').alias('y'), col('features'))

predictions = rfModel.transform(testData)

predictions.show()

from pyspark.sql.functions import col
see = predictions.select('y', col('y').alias('label'), 'rawPrediction')

see.show()

evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
print("Test: Area Under ROC: " + str(evaluator.evaluate(see, {evaluator.metricName: "areaUnderROC"})))

(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

trainingData = trainingData.select(col('prediction').alias('y'), col('features'))

trainingData.show()

from pyspark.ml.classification import NaiveBayes

# create the trainer and set its parameters
nb = NaiveBayes(labelCol="y", smoothing=1, modelType="multinomial")

# train the model
model = nb.fit(trainingData)

testData = testData.select(col('prediction').alias('y'), col('features'))

predictions = model.transform(testData)

from pyspark.sql.functions import col
see = predictions.select('y', col('y').alias('label'), 'rawPrediction')

see.show()

evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
print("Test: Area Under ROC: " + str(evaluator.evaluate(see, {evaluator.metricName: "areaUnderROC"})))

(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)
print("Training Dataset Count: " + str(trainingData.count()))
print("Test Dataset Count: " + str(testData.count()))

trainingData = trainingData.select(col('prediction').alias('y'), col('features'))

from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(labelCol="y", maxIter=20, regParam=0.3, elasticNetParam=0, family = "binomial")
# lr = LogisticRegression()

from pyspark.sql.functions import col
updatedTrainingData = trainingData.select('y', col('y').alias('label'), 'features')

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Create ParamGrid for Cross Validation
paramGrid = (ParamGridBuilder()
             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter
             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)
#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations
#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features
             .build())

# Create 5-fold CrossValidator
cv = CrossValidator(estimator=lr, \
                    estimatorParamMaps=paramGrid, \
                    evaluator=evaluator, \
                    numFolds=3)

# Run cross validations
cvModel = cv.fit(updatedTrainingData)
# this will likely take a fair amount of time because of the amount of models that we're creating and testing

testData = testData.select(col('prediction').alias('y'), col('features'))

from pyspark.sql.functions import col
updatedTestData = testData.select('y', col('y').alias('label'), 'features')

# Use test set here so we can measure the accuracy of our model on new data
predictions = cvModel.transform(updatedTestData)

# cvModel uses the best model found from the Cross Validation
# Evaluate best model
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
print("Test: Area Under ROC: " + str(evaluator.evaluate(predictions, {evaluator.metricName: "areaUnderROC"})))